{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82582e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for the project\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "data = pd.read_csv('/content/diabetes_012_health_indicators_BRFSS2021.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix for the new dataset\n",
    "\n",
    "correlation_matrix_new = data.corr()\n",
    "\n",
    "\n",
    "\n",
    "# Plot the heatmap of the correlation matrix\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(correlation_matrix_new, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "plt.title('Correlation Matrix for Diabetes Health Indicators Dataset')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Display the correlation matrix itself\n",
    "\n",
    "print(correlation_matrix_new)\n",
    "\n",
    "\n",
    "\n",
    "!pip install imbalanced-learn\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from imblearn.combine import SMOTEENN  # Import SMOTE for oversampling\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, BatchNormalization\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "data = pd.read_csv('/content/diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Selecting features and target\n",
    "\n",
    "X = data.drop(columns=['Diabetes_012'])  # All columns except the target\n",
    "\n",
    "y = data['Diabetes_012']  # Target column for prediction\n",
    "\n",
    "\n",
    "\n",
    "# Identify continuous and binary features\n",
    "\n",
    "continuous_features = ['BMI', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income']\n",
    "\n",
    "binary_features = ['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', 'HeartDiseaseorAttack',\n",
    "\n",
    "                   'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare',\n",
    "\n",
    "                   'NoDocbcCost', 'DiffWalk', 'Sex']\n",
    "\n",
    "\n",
    "\n",
    "# Scale only continuous features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X[continuous_features] = scaler.fit_transform(X[continuous_features])\n",
    "\n",
    "\n",
    "\n",
    "y = to_categorical(y)\n",
    "\n",
    "\n",
    "\n",
    "# Split the resampled data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use SMOTEENN to perform a combination of over-sampling the minority class and under-sampling the majority class\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to build a neural network model\n",
    "\n",
    "def build_model(input_dim, units_1, units_2, dropout_rate):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "    # First Dense Layer with Batch Normalization and LeakyReLU\n",
    "\n",
    "    model.add(Dense(units=units_1, input_dim=input_dim, kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "    # Second Dense Layer with Batch Normalization and LeakyReLU\n",
    "\n",
    "    model.add(Dense(units=units_2, kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "    # Third Dense Layer with Batch Normalization and LeakyReLU\n",
    "\n",
    "    model.add(Dense(units=32, kernel_initializer='he_uniform'))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "\n",
    "\n",
    "    # Output layer for multi-class classification\n",
    "\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Variables to store the best accuracy and the best model filename\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "best_model_filename = 'best_model.h5'\n",
    "\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter grid search\n",
    "\n",
    "units_1_options = [64, 128]\n",
    "\n",
    "units_2_options = [32, 64]\n",
    "\n",
    "dropout_rates = [0.2, 0.3]\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "for units_1 in units_1_options:\n",
    "\n",
    "    for units_2 in units_2_options:\n",
    "\n",
    "        for dropout_rate in dropout_rates:\n",
    "\n",
    "            # Build and train the model\n",
    "\n",
    "            model = build_model(input_dim=X_train.shape[1], units_1=units_1, units_2=units_2, dropout_rate=dropout_rate)\n",
    "\n",
    "            model.fit(X_resampled, y_resampled, epochs=epochs, batch_size=batch_size, validation_split=0.2,\n",
    "\n",
    "                      callbacks=[lr_schedule], verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "            # Evaluate the model on the test set\n",
    "\n",
    "# Make a prediction using the pre-trained model with both time-series and fine-tuned features\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "            y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate accuracy\n",
    "\n",
    "            accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "\n",
    "            print(f\"Model with {units_1} and {units_2} units, dropout {dropout_rate}: Accuracy = {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "            # If the new accuracy is better, save the model\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "\n",
    "                best_accuracy = accuracy\n",
    "\n",
    "                best_model_filename = f\"best_model_units1_{units_1}_units2_{units_2}_dropout_{dropout_rate}.h5\"\n",
    "\n",
    "                model.save(f'/content/all_features_batchnormal_lr_scheduler/{best_model_filename}')\n",
    "\n",
    "                print(f\"New best model saved as {best_model_filename} with accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Best model saved as {best_model_filename} with accuracy: {best_accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the new dataset\n",
    "\n",
    "data = pd.read_csv('/content/gestational_diabetes_dataset.csv')\n",
    "\n",
    "# load pretrained model\n",
    "\n",
    "# Load the pre-trained Keras deep learning model from the specified .h5 file\n",
    "\n",
    "pretrained_model = load_model('/content/best_model_units1_128_units2_64_dropout_0.2.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the input shape from the dataset (8 features)\n",
    "\n",
    "input_shape = (8,)\n",
    "\n",
    "\n",
    "\n",
    "# Build a new model based on the pre-trained model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "# Modify the input layer to match the new dataset\n",
    "\n",
    "model.add(Dense(128, input_shape=input_shape))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "# Reuse the rest of the pre-trained layers (we'll add them layer by layer)\n",
    "\n",
    "for layer in pretrained_model.layers[1:-1]:\n",
    "\n",
    "    model.add(layer)\n",
    "\n",
    "\n",
    "\n",
    "# Add a new output layer with 1 neuron for binary classification\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# Compile the new model\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "\n",
    "              loss='binary_crossentropy',\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "X = data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "\n",
    "y = data['Outcome']\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Train the modified model on the new dataset using transfer learning\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "\n",
    "model.save('fine_tuned_model_with_modified_input.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Select features and labels\n",
    "\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "\n",
    "X = data[features]\n",
    "\n",
    "y = data['Outcome']\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Scale the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load the pre-trained model\n",
    "\n",
    "# Load the pre-trained Keras deep learning model from the specified .h5 file\n",
    "\n",
    "model = load_model('/content/all_features_batchnormal_lr_scheduler/best_model_units1_128_units2_64_dropout_0.2.h5')\n",
    "\n",
    "\n",
    "\n",
    "# 2. Modify the input layer dimension to match the new dataset (8 features)\n",
    "\n",
    "new_input = layers.Input(shape=(X_train.shape[1],))  # Shape matches the 8 features of the new dataset\n",
    "\n",
    "old_layers = model.layers[1:]  # Exclude the original input layer\n",
    "\n",
    "\n",
    "\n",
    "# Reconnect the old layers with the new input\n",
    "\n",
    "x = new_input\n",
    "\n",
    "for layer in old_layers:\n",
    "\n",
    "    x = layer(x)\n",
    "\n",
    "\n",
    "\n",
    "# Build the new model\n",
    "\n",
    "new_model = Model(inputs=new_input, outputs=x)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Freeze all layers except the last one\n",
    "\n",
    "for layer in new_model.layers[:-1]:  # Freeze all layers except the last one\n",
    "\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "# 4. Modify the last layer for binary classification\n",
    "\n",
    "new_model.layers[-1] = layers.Dense(1, activation='sigmoid')  # Adjust this for binary classification\n",
    "\n",
    "\n",
    "\n",
    "# 5. Compile the model\n",
    "\n",
    "new_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "\n",
    "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "\n",
    "\n",
    "# Train the modified model on the new dataset\n",
    "\n",
    "history = new_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[lr_schedule], verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# Save the updated model\n",
    "\n",
    "new_model.save('/content/gestational_health_indicator_model/model.h5')\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the fine-tuned model\n",
    "\n",
    "# Load the pre-trained Keras deep learning model from the specified .h5 file\n",
    "\n",
    "fine_tuned_model = load_model('fine_tuned_model_with_modified_input.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Perform predictions on the test data\n",
    "\n",
    "# Make a prediction using the pre-trained model with both time-series and fine-tuned features\n",
    "\n",
    "y_pred = fine_tuned_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# Convert probabilities to binary outcome (0 or 1)\n",
    "\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few predictions and their corresponding true values\n",
    "\n",
    "predictions_comparison = pd.DataFrame({'True Outcome': y_test, 'Predicted Outcome': y_pred_binary.flatten()})\n",
    "\n",
    "print(predictions_comparison.head())\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "test_loss, test_accuracy = fine_tuned_model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Print out the test accuracy and loss\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "file_path = 'BIT_2019.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# Function to classify diabetes status based on Diabetic, Pdiabetes, and Pregancies columns\n",
    "\n",
    "def classify_diabetes(row):\n",
    "\n",
    "    if row['Diabetic'] == 'no' and row['Pdiabetes'] == 0:\n",
    "\n",
    "        return 'No Diabetes'\n",
    "\n",
    "    elif row['Pdiabetes'] == 1:\n",
    "\n",
    "        return 'Prediabetes'\n",
    "\n",
    "    elif row['Diabetic'] == 'yes' and row['Pregancies'] == 0:\n",
    "\n",
    "        return 'Type-2 Diabetes'\n",
    "\n",
    "    elif row['Diabetic'] == 'yes' and row['Pregancies'] > 0:\n",
    "\n",
    "        return 'Gestational Diabetes'\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 'Unknown'\n",
    "\n",
    "\n",
    "\n",
    "# Fill missing values in 'Pdiabetes' and 'Diabetic' columns\n",
    "\n",
    "data['Pdiabetes'].fillna('0', inplace=True)  # Assuming no prediabetes for missing values\n",
    "\n",
    "data['Diabetic'].fillna('no', inplace=True)  # Assuming no diabetes for missing values\n",
    "\n",
    "\n",
    "\n",
    "# Standardize the 'Pdiabetes' column to binary format (0 for no, 1 for yes)\n",
    "\n",
    "data['Pdiabetes'] = data['Pdiabetes'].replace({'yes': 1, 'no': 0, '0': 0}).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Apply the classification function to create the target column 'Diabetes_Status'\n",
    "\n",
    "data['Diabetes_Status'] = data.apply(classify_diabetes, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Check the distribution of the newly created 'Diabetes_Status' column\n",
    "\n",
    "print(data['Diabetes_Status'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# Drop rows where 'Diabetes_Status' is 'Unknown'\n",
    "\n",
    "data_cleaned = data[data['Diabetes_Status'] != 'Unknown']\n",
    "\n",
    "\n",
    "\n",
    "# Check the updated distribution after dropping 'Unknown' cases\n",
    "\n",
    "print(data_cleaned['Diabetes_Status'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Drop the 'Diabetic' column as it's redundant\n",
    "\n",
    "X = data_cleaned.drop(['Diabetic', 'Diabetes_Status'], axis=1)\n",
    "\n",
    "y = data_cleaned['Diabetes_Status']\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to convert age ranges to numeric values (e.g., using the midpoint of the range)\n",
    "\n",
    "def convert_age_range(age_range):\n",
    "\n",
    "    if isinstance(age_range, str) and '-' in age_range:\n",
    "\n",
    "        age_min, age_max = age_range.split('-')\n",
    "\n",
    "        return (int(age_min) + int(age_max)) // 2\n",
    "\n",
    "    else:\n",
    "\n",
    "        return pd.to_numeric(age_range, errors='coerce')  # Handle any non-range values\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function to the Age column\n",
    "\n",
    "X['Age'] = X['Age'].apply(convert_age_range)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets before scaling\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# One-hot encode categorical columns and scale numerical columns\n",
    "\n",
    "categorical_features = ['Gender', 'Family_Diabetes', 'highBP', 'PhysicallyActive', 'Smoking', 'Alcohol', 'BPLevel']\n",
    "\n",
    "numerical_features = ['Age', 'BMI', 'Pregancies', 'Sleep', 'SoundSleep']\n",
    "\n",
    "\n",
    "\n",
    "# Create a preprocessor that scales numerical data and one-hot encodes categorical data\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\n",
    "    transformers=[\n",
    "\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Fit the preprocessor on the training data and transform both the training and test data and saving it for training data from Streamlit app\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "joblib.dump(preprocessor, 'preprocesser.pkl')\n",
    "\n",
    "\n",
    "\n",
    "# Handle missing values using SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "X_train_imputed = imputer.fit_transform(X_train_processed)\n",
    "\n",
    "\n",
    "\n",
    "# Encode the target variable (y_train) using LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Check class distribution before applying SMOTE\n",
    "\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train_encoded))\n",
    "\n",
    "\n",
    "\n",
    "# Apply SMOTE to the imputed training data\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_imputed, y_train_encoded)\n",
    "\n",
    "\n",
    "\n",
    "# Check class distribution after applying SMOTE\n",
    "\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_smote))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "X_train_smote_df = pd.DataFrame(X_train_smote, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Generate the correlation matrix\n",
    "\n",
    "corr_matrix = X_train_smote_df.corr()\n",
    "\n",
    "\n",
    "\n",
    "# Plot the correlation matrix\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(corr_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "# Train a Random Forest model to evaluate feature importance\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "# Get feature importances\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "feature_names = X_train_smote_df.columns\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "# Display the most important features\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "print(feature_importance_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "# Keep the top 15 most important features\n",
    "\n",
    "top_n = 15\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "top_features = feature_importance_df.head(top_n)['Feature']\n",
    "\n",
    "\n",
    "\n",
    "# Filter the dataset to keep only the top features\n",
    "\n",
    "X_train_top_features = X_train_smote_df[top_features]\n",
    "\n",
    "X_test_top_features = X_test_processed[:, :len(top_features)]  # Ensure the test set matches the selected features\n",
    "\n",
    "\n",
    "\n",
    "''' Top Features '''\n",
    "\n",
    "'''\n",
    "\n",
    "num__Pregancies (0.205519)\n",
    "\n",
    "num__BMI (0.095190)\n",
    "\n",
    "num__SoundSleep (0.081070)\n",
    "\n",
    "num__Sleep (0.071806)\n",
    "\n",
    "cat__Gender_Male (0.058135)\n",
    "\n",
    "num__Age (0.053376)\n",
    "\n",
    "cat__Gender_Female (0.052271)\n",
    "\n",
    "cat__Family_Diabetes_yes (0.044955)\n",
    "\n",
    "cat__PhysicallyActive_less than half an hr (0.044192)\n",
    "\n",
    "cat__PhysicallyActive_none (0.038670)\n",
    "\n",
    "cat__Family_Diabetes_no (0.037938)\n",
    "\n",
    "cat__BPLevel_high (0.037674)\n",
    "\n",
    "cat__highBP_no (0.034218)\n",
    "\n",
    "cat__highBP_yes (0.032206)\n",
    "\n",
    "cat__BPLevel_normal (0.027395)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the fine-tuned model\n",
    "\n",
    "# Load the pre-trained Keras deep learning model from the specified .h5 file\n",
    "\n",
    "fine_tuned_model = load_model('fine_tuned_model_with_modified_input.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Number of classes (No Diabetes, Prediabetes, Type-2 Diabetes, Gestational Diabetes)\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "\n",
    "\n",
    "# Build a new model for fine-tuning with multi-class output\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "# Modify the input layer to accept only 15 features\n",
    "\n",
    "model.add(Dense(128, input_shape=(15,), activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "# Manually add the remaining layers from the pre-trained model with unique names\n",
    "\n",
    "for i, layer in enumerate(fine_tuned_model.layers[1:-1]):  # Exclude the last layer\n",
    "\n",
    "    if isinstance(layer, BatchNormalization):  # Handle BatchNormalization\n",
    "\n",
    "        model.add(BatchNormalization(name=f\"{layer.name}_{i}\"))\n",
    "\n",
    "    elif isinstance(layer, Dropout):  # Handle Dropout\n",
    "\n",
    "        model.add(Dropout(rate=layer.rate, name=f\"{layer.name}_{i}\"))\n",
    "\n",
    "    elif isinstance(layer, LeakyReLU):  # Handle LeakyReLU activation layer without passing alpha\n",
    "\n",
    "        model.add(LeakyReLU(name=f\"{layer.name}_{i}\"))\n",
    "\n",
    "    elif isinstance(layer, Dense):  # Handle Dense layers\n",
    "\n",
    "        model.add(Dense(units=layer.units, activation=layer.activation, name=f\"{layer.name}_{i}\"))\n",
    "\n",
    "\n",
    "\n",
    "# Add a new output layer for multi-class classification (4 classes)\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Softmax for multi-class output\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss (for label-encoded targets)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),  # Lower learning rate for fine-tuning\n",
    "\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "print(X_train_top_features.dtypes)  # Check if all columns are numerical\n",
    "\n",
    "\n",
    "\n",
    "print(X_train_top_features.shape)  # Should be (num_samples, 15)\n",
    "\n",
    "print(X_test_top_features.shape)   # Should be (num_samples, 15)\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the features and labels are NumPy arrays\n",
    "\n",
    "X_train_top_features = np.array(X_train_top_features)\n",
    "\n",
    "X_test_top_features = np.array(X_test_top_features)\n",
    "\n",
    "y_train_smote = np.array(y_train_smote)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(y_train_smote.shape)  # Should be (num_samples,)\n",
    "\n",
    "print(y_test.shape)         # Should be (num_samples,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train_top_features.shape)  # Expected shape: (num_samples, 15)\n",
    "\n",
    "print(X_test_top_features.shape)   # Expected shape: (num_samples, 15)\n",
    "\n",
    "print(y_train_smote.shape)         # Expected shape: (num_samples,)\n",
    "\n",
    "print(y_test.shape)                # Expected shape: (num_samples,)\n",
    "\n",
    "\n",
    "\n",
    "# Reshape labels to 1D if needed\n",
    "\n",
    "y_train_smote = y_train_smote.reshape(-1)\n",
    "\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "# Check the model's output layer\n",
    "\n",
    "model.summary()  # Ensure the output layer has 4 units and uses softmax\n",
    "\n",
    "\n",
    "\n",
    "print(np.unique(y_train_smote))  # Check unique values in y_train_smote\n",
    "\n",
    "print(np.unique(y_test))         # Check unique values in y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "# Fit on the training labels to ensure consistent encoding\n",
    "\n",
    "label_encoder.fit(['No Diabetes', 'Prediabetes', 'Type-2 Diabetes', 'Gestational Diabetes'])\n",
    "\n",
    "\n",
    "\n",
    "# Transform the test labels\n",
    "\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model on the label-encoded target variable\n",
    "\n",
    "# Train the model\n",
    "\n",
    "history = model.fit(X_train_top_features, y_train_smote,\n",
    "\n",
    "                    validation_data=(X_test_top_features, y_test_encoded),\n",
    "\n",
    "                    epochs=30, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_top_features, y_test_encoded)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "\n",
    "model.save('fine_tuned_model_on_new_dataset.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv('merged_CGM_clinical_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "df['Hora'] = pd.to_timedelta(df['Hora']).dt.total_seconds()\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "numerical_cols = ['Hora', 'Glucemia', 'BMI', 'age', 'HbA1c', 'follow.up']\n",
    "\n",
    "\n",
    "\n",
    "# Scale the numerical columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "\n",
    "\n",
    "# saving scaler for time series data in Streamlit app\n",
    "\n",
    "joblib.dump(scaler, 'time_series_scaler.pkl')\n",
    "\n",
    "\n",
    "\n",
    "# Handle class imbalance by oversampling the minority class\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "\n",
    "# Check class distribution\n",
    "\n",
    "df_majority = df[df.T2DM == False]\n",
    "\n",
    "df_minority = df[df.T2DM == True]\n",
    "\n",
    "\n",
    "\n",
    "# Upsample minority class\n",
    "\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "\n",
    "                                 replace=True,     # sample with replacement\n",
    "\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "\n",
    "                                 random_state=42)  # reproducible results\n",
    "\n",
    "\n",
    "\n",
    "# Combine majority and upsampled minority class\n",
    "\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the features and target\n",
    "\n",
    "X_data = df_upsampled[numerical_cols].values\n",
    "\n",
    "y_data = df_upsampled['T2DM'].astype(int).values\n",
    "\n",
    "\n",
    "\n",
    "# Set the number of timesteps (window size)\n",
    "\n",
    "window_size = 10  # Adjust this to the number of timesteps you want\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to create sequences of data (sliding window approach)\n",
    "\n",
    "def create_sequences(data, target, window_size):\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - window_size):\n",
    "\n",
    "        X.append(data[i:i+window_size])\n",
    "\n",
    "        y.append(target[i+window_size])  # Predict the value after the window\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "# Create sequences\n",
    "\n",
    "X, y = create_sequences(X_data, y_data, window_size)\n",
    "\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Build the LSTM model with regularization and dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))  # Define input shape with multiple timesteps\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)))  # L2 regularization\n",
    "\n",
    "model.add(Dropout(0.3))  # Increased dropout to prevent overfitting\n",
    "\n",
    "model.add(LSTM(64, kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(25, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "\n",
    "                    validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "# Make a prediction using the pre-trained model with both time-series and fine-tuned features\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the accuracy and loss curves\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
    "\n",
    "plt.title('Accuracy Curve')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='test loss')\n",
    "\n",
    "plt.title('Loss Curve')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No T2DM', 'T2DM'], yticklabels=['No T2DM', 'T2DM'])\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save('/content/time_series_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "# Load the two models\n",
    "\n",
    "model_1_path = '/content/time_series_model.h5'\n",
    "\n",
    "model_2_path = '/content/fine_tuned_model_on_new_dataset.h5'\n",
    "\n",
    "\n",
    "\n",
    "# Load models\n",
    "\n",
    "model_1 = keras.models.load_model(model_1_path)\n",
    "\n",
    "model_2 = keras.models.load_model(model_2_path)\n",
    "\n",
    "\n",
    "\n",
    "# Summarize the models to analyze their architectures\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "\n",
    "\n",
    "model_2.input_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.layers import Input, concatenate, Dense\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "# Define input shapes based on the original models\n",
    "\n",
    "# Collect CGM (Continuous Glucose Monitoring) data from the user as a comma-separated string\n",
    "\n",
    "time_series_input_shape = model_1.input_shape[1:]  # Exclude batch size\n",
    "\n",
    "fine_tuned_input_shape = model_2.input_shape[1:]   # Exclude batch size\n",
    "\n",
    "\n",
    "\n",
    "# Create new input layers for both models\n",
    "\n",
    "# Collect CGM (Continuous Glucose Monitoring) data from the user as a comma-separated string\n",
    "\n",
    "time_series_input = Input(shape=time_series_input_shape)\n",
    "\n",
    "fine_tuned_input = Input(shape=fine_tuned_input_shape)\n",
    "\n",
    "\n",
    "\n",
    "# Call each model on its respective input\n",
    "\n",
    "# Collect CGM (Continuous Glucose Monitoring) data from the user as a comma-separated string\n",
    "\n",
    "time_series_output = model_1(time_series_input)\n",
    "\n",
    "fine_tuned_output = model_2(fine_tuned_input)\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate the outputs from the two models\n",
    "\n",
    "combined_features = concatenate([time_series_output, fine_tuned_output])\n",
    "\n",
    "\n",
    "\n",
    "# Add a new output layer for the combined model\n",
    "\n",
    "new_output = Dense(4, activation='softmax')(combined_features)  # 4 classes\n",
    "\n",
    "\n",
    "\n",
    "# Create the new combined model\n",
    "\n",
    "# Collect CGM (Continuous Glucose Monitoring) data from the user as a comma-separated string\n",
    "\n",
    "combined_model = Model(inputs=[time_series_input, fine_tuned_input], outputs=new_output)\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model (use suitable optimizer and loss for multiclass classification)\n",
    "\n",
    "combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Summarize the new combined model\n",
    "\n",
    "combined_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "combined_model.save('/content/final_deep_learning_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "# Load the combined model\n",
    "\n",
    "# Load the pre-trained Keras deep learning model from the specified .h5 file\n",
    "\n",
    "combined_model = load_model('final_deep_learning_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Get the input shapes of the combined model\n",
    "\n",
    "# Collect CGM (Continuous Glucose Monitoring) data from the user as a comma-separated string\n",
    "\n",
    "time_series_input_shape = combined_model.input[0].shape\n",
    "\n",
    "fine_tuned_input_shape = combined_model.input[1].shape\n",
    "\n",
    "\n",
    "\n",
    "# Collect CGM (Continuous Glucose Monitoring) data from the user as a comma-separated string\n",
    "\n",
    "print(f\"Time Series Input Shape: {time_series_input_shape}\")\n",
    "\n",
    "print(f\"Fine-Tuned Input Shape: {fine_tuned_input_shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Importing necessary libraries for the project\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "time_series_data = pd.read_csv('/content/merged_CGM_clinical_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few rows to check the structure\n",
    "\n",
    "print(time_series_data.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
